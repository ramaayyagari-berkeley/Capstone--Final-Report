{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e1784e5-f7eb-4b87-a70b-3ef240ef4aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import multiprocessing\n",
    "# utility functions\n",
    "import json\n",
    "import os\n",
    "from ipynb.fs.full.utilities import BERTModel, download_extract, Vocab, load_data_snli, try_all_gpus, TokenEmbedding, train_and_evaluate, get_dataloader_workers, SNLIBERTDataset, read_snli, BERTClassifier, display_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fa0761b-75b7-4e77-942b-9658e2100e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---> will run on nividia cuda gpu(s) count 1\n"
     ]
    }
   ],
   "source": [
    "# global variables\n",
    "devices = try_all_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5ec82a-1648-44b3-9a6e-3ad242c3efac",
   "metadata": {},
   "source": [
    "#### Natural Language Inference with `Decomposable attention model with MLPs(Multi Layer Perceptrons) and GloVe embeddings`\n",
    "![Solution Overview](images/NaturalLanguageInference.GloVe.png)\n",
    "###### The solution will feed pretrained [GloVe](GLOVE.md) embeddings to [MLP](MLP.md) and [Attention](ATTENTION.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78341935-6786-4276-bfff-b48fc8b050fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(num_inputs, num_hiddens, flatten):\n",
    "    \"\"\"Inputs are separately taken rather than takes a pair of them together.\n",
    "       This decomposition trick leads to only m+n applications (linear complexity) \n",
    "       rather than m*n applications (quadratic complexity).\n",
    "    \"\"\"\n",
    "    net = []\n",
    "    net.append(nn.Dropout(0.2))\n",
    "    net.append(nn.Linear(num_inputs, num_hiddens))\n",
    "    net.append(nn.ReLU())\n",
    "    if flatten:\n",
    "        net.append(nn.Flatten(start_dim=1))\n",
    "    net.append(nn.Dropout(0.2))\n",
    "    net.append(nn.Linear(num_hiddens, num_hiddens))\n",
    "    net.append(nn.ReLU())\n",
    "    if flatten:\n",
    "        net.append(nn.Flatten(start_dim=1))\n",
    "    return nn.Sequential(*net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "338609af-9771-48fd-a238-b2dcf09695e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attend(nn.Module):\n",
    "    \"\"\"Compute the soft alignment of hypotheses (beta) withinput premises A and \n",
    "       soft alignment of premises (alpha) with input hypotheses B.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs, num_hiddens, **kwargs):\n",
    "        super(Attend, self).__init__(**kwargs)\n",
    "        self.f = mlp(num_inputs, num_hiddens, flatten=False)\n",
    "    def forward(self, A, B):\n",
    "        # Shape of `A`/`B`: (`batch_size`, no. of tokens in sequence A/B,\n",
    "        # `embed_size`)\n",
    "        # Shape of `f_A`/`f_B`: (`batch_size`, no. of tokens in sequence A/B,\n",
    "        # `num_hiddens`)\n",
    "        f_A = self.f(A)\n",
    "        f_B = self.f(B)\n",
    "        # Shape of `e`: (`batch_size`, no. of tokens in sequence A,\n",
    "        # no. of tokens in sequence B)\n",
    "        e = torch.bmm(f_A, f_B.permute(0, 2, 1))\n",
    "        # Shape of `beta`: (`batch_size`, no. of tokens in sequence A,\n",
    "        # `embed_size`), where sequence B is softly aligned with each token\n",
    "        # (axis 1 of `beta`) in sequence A\n",
    "        beta = torch.bmm(F.softmax(e, dim=-1), B)\n",
    "        # Shape of `alpha`: (`batch_size`, no. of tokens in sequence B,\n",
    "        # `embed_size`), where sequence A is softly aligned with each token\n",
    "        # (axis 1 of `alpha`) in sequence B\n",
    "        alpha = torch.bmm(F.softmax(e.permute(0, 2, 1), dim=-1), A)\n",
    "        return beta, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae3090eb-9c36-487d-bdf0-a7d146ff496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compare(nn.Module):\n",
    "    \"\"\"Compare between token 𝑖 in the premise and all the hypothesis tokens that are softly aligned with token 𝑖 and \n",
    "       compare between token 𝑗 in the hypothesis and all the premise tokens that are softly aligned with token 𝑗.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs, num_hiddens, **kwargs):\n",
    "        super(Compare, self).__init__(**kwargs)\n",
    "        self.g = mlp(num_inputs, num_hiddens, flatten=False)\n",
    "    def forward(self, A, B, beta, alpha):\n",
    "        V_A = self.g(torch.cat([A, beta], dim=2))\n",
    "        V_B = self.g(torch.cat([B, alpha], dim=2))\n",
    "        return V_A, V_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "056861e7-4ca1-4cbf-ad39-816d05fe422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aggregate(nn.Module):\n",
    "    \"\"\"With two comparision vectors as input, aggregate that information to infer the logical relationship.\n",
    "       This involes summing up both vectors and feed the concatenation of both summarization results into \n",
    "       function ℎ (an MLP) to obtain the classification result of the logical relationship.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs, num_hiddens, num_outputs, **kwargs):\n",
    "        super(Aggregate, self).__init__(**kwargs)\n",
    "        self.h = mlp(num_inputs, num_hiddens, flatten=True)\n",
    "        self.linear = nn.Linear(num_hiddens, num_outputs)\n",
    "    def forward(self, V_A, V_B):\n",
    "        # Sum up both sets of comparison vectors\n",
    "        V_A = V_A.sum(dim=1)\n",
    "        V_B = V_B.sum(dim=1)\n",
    "        # Feed the concatenation of both summarization results into an MLP\n",
    "        Y_hat = self.linear(self.h(torch.cat([V_A, V_B], dim=1)))\n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83e74f5c-41ff-437f-95b4-f63db078e586",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecomposableAttention(nn.Module):\n",
    "    \"\"\"Model to jointly train attention, comparing and aggregating steps together.\"\"\"\n",
    "    def __init__(self, vocab, embed_size, num_hiddens, num_inputs_attend=100, num_inputs_compare=200, num_inputs_agg=400, **kwargs):\n",
    "        super(DecomposableAttention, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        self.attend = Attend(num_inputs_attend, num_hiddens)\n",
    "        self.compare = Compare(num_inputs_compare, num_hiddens)\n",
    "        # There are 3 possible outputs: entailment, contradiction, and neutral\n",
    "        self.aggregate = Aggregate(num_inputs_agg, num_hiddens, num_outputs=3)\n",
    "    def forward(self, X):\n",
    "        premises, hypotheses = X\n",
    "        A = self.embedding(premises)\n",
    "        B = self.embedding(hypotheses)\n",
    "        beta, alpha = self.attend(A, B)\n",
    "        V_A, V_B = self.compare(A, B, beta, alpha)\n",
    "        Y_hat = self.aggregate(V_A, V_B)\n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ea34cf0-4218-46da-9144-6bc530c881f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---> snli data already extracted at ../data\\snli_1.0\n",
      " ---> read 549367 examples\n",
      " ---> read 9824 examples\n"
     ]
    }
   ],
   "source": [
    "# Reading the dataset\n",
    "batch_size, num_steps = 256, 50\n",
    "train_iter, test_iter, vocab, data_dir = load_data_snli(batch_size, num_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfed11d-1106-45ea-96ed-361ccd1350a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Model\n",
    "embed_size, num_hiddens = 100, 200\n",
    "net = DecomposableAttention(vocab, embed_size, num_hiddens)\n",
    "glove_embedding = TokenEmbedding('glove.6b.100d')\n",
    "embeds = glove_embedding[vocab.idx_to_token]\n",
    "net.embedding.weight.data.copy_(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fba2cc7-e414-43f1-b821-bb4c7199ee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and evaluating the model\n",
    "lr, num_epochs = 0.001, 4\n",
    "trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "results = train_and_evaluate(net, train_iter, test_iter, loss, trainer, num_epochs, devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2595da72-975a-44bd-ad71-97969b5a7146",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_results(results, \"glove_embedding_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75420f44-513c-48b9-8def-c1c23662bf1e",
   "metadata": {},
   "source": [
    "#### Natural Language Inference with `MLP(Multi Layer Perceptrons) and BERT(Bidirectional Encoder Representations from Transformers) embeddings`\n",
    "![Solution Overview](images/NaturalLanguageInference.BERT.png)\n",
    "###### The solution will feed pretrained [BERT](BERT.md) embeddings to [MLP](MLP.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea8b923-165b-448f-a408-080be3e77451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # bert.small --> for development\n",
    "# bert_type =\"bert.small\"\n",
    "# num_hiddens = 256\n",
    "# ffn_num_hiddens = 512\n",
    "# num_heads = 4\n",
    "# num_blks = 2\n",
    "# # bert.base --> for evaluation and verification *** takes time ***\n",
    "bert_type =\"bert.base\"\n",
    "num_hiddens = 768\n",
    "ffn_num_hiddens = 3072\n",
    "num_heads = 12\n",
    "num_blks = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c74d81f-bb88-49ca-ba22-6c42f6f35fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens, num_heads, num_blks, dropout, max_len, devices):\n",
    "    data_dir = download_extract(pretrained_model)\n",
    "    # Define an empty vocabulary to load the predefined vocabulary\n",
    "    vocab = Vocab()\n",
    "    vocab.idx_to_token = json.load(open(os.path.join(data_dir, 'vocab.json')))\n",
    "    vocab.token_to_idx = {token: idx for idx, token in enumerate(\n",
    "    vocab.idx_to_token)}\n",
    "    bert = BERTModel(len(vocab), num_hiddens, ffn_num_hiddens=ffn_num_hiddens, num_heads=num_heads,num_blks=num_blks, dropout=dropout, max_len=max_len)\n",
    "    # Load pretrained BERT parameters\n",
    "    bert.load_state_dict(torch.load(os.path.join(data_dir, 'pretrained.params')))\n",
    "    return bert, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da22adb-abd7-41ad-b674-017f36a72ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained BERT \n",
    "bert_model, vocab = load_pretrained_model(bert_type, num_hiddens=num_hiddens, ffn_num_hiddens=ffn_num_hiddens, num_heads=num_heads, num_blks=num_blks, dropout=0.1, max_len=512, devices=devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24615242-48b2-4ebc-9163-70eb079a9006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for fine tuning BERT\n",
    "# Reduce `batch_size` if there is an out of memory error. In the original BERT model, `max_len` = 512\n",
    "batch_size, max_len, num_workers = 512, 128, get_dataloader_workers()\n",
    "train_set = SNLIBERTDataset(read_snli(data_dir, True), max_len, vocab)\n",
    "test_set = SNLIBERTDataset(read_snli(data_dir, False), max_len, vocab)\n",
    "train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_iter = torch.utils.data.DataLoader(test_set, batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab94a3a0-b047-4bca-b0eb-24b671c65404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tuning BERT\n",
    "bert_net = BERTClassifier(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b3f9e5-07ff-4742-9065-43cdac62c2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Risky *** , system may be get crashed, but needed to run for bert.base embeddings\n",
    "if bert_type == 'bert.base':\n",
    "    if devices[0].type == \"cuda\":\n",
    "        torch.cuda.set_per_process_memory_fraction(0.0)\n",
    "    elif devices[0].type == \"mps\":\n",
    "        torch.mps.set_per_process_memory_fraction(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9380ffe7-7c5c-4b98-b759-32ef58199150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and evaluate Model performance\n",
    "lr, num_epochs = 1e-4, 5\n",
    "trainer = torch.optim.Adam(bert_net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "bert_net(next(iter(train_iter))[0])\n",
    "results = train_and_evaluate(bert_net, train_iter, test_iter, loss, trainer, num_epochs, devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c10fd15-410c-4978-9f31-b3ef42fad5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display results\n",
    "display_results(results, f'{bert_type}_embedding_results')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
